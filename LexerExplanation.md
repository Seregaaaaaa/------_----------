# Детальное описание лексического анализатора NutScript

## Введение

Лексический анализ — это первый этап работы компилятора. Его основная задача — преобразовать последовательность символов исходного кода в последовательность *лексем* (токенов). Каждая лексема представляет собой минимальную смысловую единицу языка, например, ключевое слово, идентификатор, оператор или константа.

Данный лексический анализатор разработан для языка NutScript и строго следует [документации по лексемам и автомату](Методы%20компиляции%20(2).md).

## Основные компоненты

Лексический анализатор состоит из нескольких ключевых файлов:

1.  **`token.py`**: Определяет структуру лексемы.
2.  **`token_type.py`**: Содержит перечисление всех возможных типов лексем.
3.  **`state.py`**: Определяет состояния конечного автомата, используемого лексером.
4.  **`lexer.py`**: Содержит основную логику лексического анализатора.

Рассмотрим каждый из них подробнее.

### 1. Структура Лексемы (`token.py`)

Файл `src/token.py` определяет класс `Token`, который используется для представления каждой распознанной лексемы.

```python
# src/token.py
class Token:
    def __init__(self, token_type, value, line, position):
        self.token_type = token_type  # Тип лексемы (из TokenType)
        self.value = value            # Значение лексемы (например, имя переменной или число)
        self.line = line              # Номер строки, где найдена лексема
        self.position = position      # Позиция (индекс символа) в строке

    def __str__(self):
        if self.value:
            return f"{self.token_type}: {self.value}"
        return f"{self.token_type}"
```

Каждый объект `Token` хранит:
*   `token_type`: Тип лексемы, являющийся одним из значений перечисления `TokenType`.
*   `value`: Необязательное строковое представление значения лексемы. Например, для идентификатора это будет его имя, для числовой константы — само число. Для ключевых слов или операторов значение может отсутствовать или дублировать сам символ оператора.
*   `line`: Номер строки в исходном коде, где начинается лексема. Это полезно для сообщений об ошибках.
*   `position`: Начальная позиция лексемы в строке. Также используется для указания места ошибки.

### 2. Типы Лексем (`token_type.py`)

Файл `src/token_type.py` определяет перечисление `TokenType`, которое содержит все возможные типы лексем, поддерживаемые языком NutScript. Эти типы строго соответствуют [списку лексем из документации](Методы%20компиляции%20(2).md#Список-лексем).

```python
# src/token_type.py
from enum import IntEnum

class TokenType(IntEnum):
    # Служебные операторы (1-7)
    INT = 1
    FLOAT = 2
    IF = 3
    # ... (остальные типы) ...
    UNARY_MINUS = 41 # ~

    # Специальные символы (90-100)
    WHITESPACE = 90 # Не используется напрямую в списке токенов, но обрабатывается
    NEWLINE = 91    # Аналогично WHITESPACE
    EOF = 99        # Конец файла
    ERROR = 100     # Ошибка лексического анализа
```
Каждому типу лексемы присвоен уникальный целочисленный код, как указано в документации. Это может быть полезно для последующих этапов компиляции (например, для синтаксического анализатора).

**Примеры типов лексем:**
*   `INT`, `FLOAT`: Ключевые слова для объявления типов данных.
*   `IF`, `ELSE`, `WHILE`: Управляющие конструкции.
*   `IDENTIFIER`: Имена переменных, функций и т.д.
*   `INTEGER_CONST`, `FLOAT_CONST`: Числовые константы.
*   `PLUS`, `MINUS`, `ASSIGN`: Арифметические операторы и оператор присваивания.
*   `LPAREN`, `RPAREN`, `SEMICOLON`: Скобки и разделители.
*   `EOF`: Специальный токен, обозначающий конец входного потока символов.
*   `ERROR`: Указывает на лексическую ошибку.

### 3. Состояния Автомата (`state.py`)

Файл `src/state.py` определяет перечисление `State`, представляющее состояния конечного автомата, на котором основан лексический анализатор. Эти состояния также соответствуют [таблице переходов автомата из документации](Методы%20компиляции%20(2).md#Таблица-переходов-конечного-автомата).

```python
# src/state.py
from enum import Enum, auto

class State(Enum):
    S = auto()        # Начальное состояние
    A = auto()        # Распознавание идентификатора/ключевого слова
    B = auto()        # Распознавание целой части числа
    C = auto()        # Состояние после точки в числе (ожидание дробной части)
    D = auto()        # Распознавание дробной части числа
    # E - неявное состояние, когда токен распознан и добавлен (для односимвольных)
    # E_STAR - неявное состояние, когда многосимвольный токен завершен
    Z = auto()        # Состояние ошибки
```

**Описание состояний:**
*   `S`: Начальное состояние. Лексер находится в этом состоянии, когда ожидает начало новой лексемы.
*   `A`: Состояние распознавания идентификатора или ключевого слова. Лексер переходит в это состояние, встретив букву.
*   `B`: Состояние распознавания целой части числа. Лексер переходит сюда, встретив цифру.
*   `C`: Состояние после точки в числе. Ожидается как минимум одна цифра для формирования дробной части.
*   `D`: Состояние распознавания дробной части вещественного числа.
*   `Z`: Состояние ошибки. Лексер переходит сюда при обнаружении недопустимой последовательности символов.

Состояния `E` (правильное конечное состояние для односимвольных лексем) и `E_STAR` (завершение многосимвольной лексемы с возможным возвратом символа в поток) реализованы через логику переходов в `lexer.py` и не выделены как отдельные явные состояния в `Enum`, так как после них анализатор всегда возвращается в состояние `S` или обрабатывает следующий символ.

### 4. Лексический Анализатор (`lexer.py`)

Файл `src/lexer.py` содержит класс `Lexer`, который реализует сам процесс лексического анализа.

```python
# src/lexer.py
from .state import State
from .token import Token
from .token_type import TokenType

class Lexer:
    OPERATORS = {
        '+': TokenType.PLUS,      # 30
        # ... другие операторы ...
        '.': TokenType.DOT        # 28
    }

    KEYWORDS = {
        'int': TokenType.INT,     # 1
        # ... другие ключевые слова ...
        'input': TokenType.INPUT   # 7
    }

    def __init__(self):
        self.state = State.S
        self.buffer = ""          # Буфер для накопления символов текущей лексемы
        self.tokens = []          # Список для хранения распознанных лексем
        self.line = 1             # Текущий номер строки
        self.position = 0         # Текущая позиция в строке (индекс символа)
        self.start_position = 0   # Начальная позиция текущей лексемы в строке

    def add_token(self, token_type, value=""):
        # Добавляет новый токен в список
        self.tokens.append(Token(token_type, value, self.line, self.start_position))

    def analyze(self, input_string):
        input_string += '\0'  # Добавляем маркер конца файла (EOF)
        i = 0
        while i < len(input_string):
            c = input_string[i]
            self.current_char_position = self.position # Запоминаем позицию для токена

            if self.state == State.S:
                self.start_position = self.position # Новая лексема начинается здесь
                if c.isalpha():
                    self.state = State.A
                    self.buffer = c
                elif c.isdigit():
                    self.state = State.B
                    self.buffer = c
                elif c in self.OPERATORS:
                    # Односимвольный оператор или разделитель
                    self.add_token(self.OPERATORS[c], c)
                    self.state = State.S # Возвращаемся в начальное состояние
                elif c in [' ', '\t']:
                    # Пропускаем пробелы и табы
                    self.state = State.S
                elif c == '\n':
                    # Переход на новую строку
                    self.line += 1
                    self.position = -1 # Позиция станет 0 после инкремента в конце цикла
                    self.state = State.S
                elif c == '\0': # Специальный символ конца файла
                    self.add_token(TokenType.EOF)
                    break # Завершаем анализ
                else:
                    # Недопустимый символ в начальном состоянии
                    self.state = State.Z
                    # self.add_token(TokenType.ERROR, c) # Можно добавить токен ошибки
                    raise RuntimeError(f"Неизвестный символ '{c}' в строке {self.line}, позиция {self.current_char_position}")

            elif self.state == State.A: # Распознавание идентификатора/ключевого слова
                if c.isalnum():
                    self.buffer += c
                else:
                    # Идентификатор или ключевое слово завершено
                    token_type = self.KEYWORDS.get(self.buffer, TokenType.IDENTIFIER)
                    self.add_token(token_type, self.buffer)
                    self.buffer = ""
                    self.state = State.S
                    i -= 1 # Повторно обработать текущий символ c в состоянии S
            
            elif self.state == State.B: # Распознавание целой части числа
                if c.isdigit():
                    self.buffer += c
                elif c == '.':
                    # Возможен переход к вещественному числу
                    self.buffer += c
                    self.state = State.C
                else:
                    # Целое число завершено
                    self.add_token(TokenType.INTEGER_CONST, self.buffer)
                    self.buffer = ""
                    self.state = State.S
                    i -= 1 # Повторно обработать текущий символ c в состоянии S

            elif self.state == State.C: # Состояние после точки (ожидание цифры)
                if c.isdigit():
                    self.buffer += c
                    self.state = State.D
                else:
                    # Ошибка: после точки должна быть цифра
                    self.state = State.Z
                    # self.add_token(TokenType.ERROR, self.buffer + c)
                    raise RuntimeError(f"Ожидалась цифра после точки для вещественного числа в строке {self.line}, позиция {self.current_char_position}. Получено: '{self.buffer+c}'")

            elif self.state == State.D: # Распознавание дробной части
                if c.isdigit():
                    self.buffer += c
                else:
                    # Вещественное число завершено
                    self.add_token(TokenType.FLOAT_CONST, self.buffer)
                    self.buffer = ""
                    self.state = State.S
                    i -= 1 # Повторно обработать текущий символ c в состоянии S
            
            elif self.state == State.Z: # Состояние ошибки
                # Обычно ошибка уже была сгенерирована при переходе в Z
                # Можно добавить дополнительную логику пропуска до следующего разделителя
                raise RuntimeError(f"Лексическая ошибка (состояние Z) в строке {self.line}, позиция {self.current_char_position}")

            i += 1
            self.position += 1
        
        # Если после цикла остался буфер (например, файл закончился сразу после идентификатора)
        if self.buffer:
            if self.state == State.A:
                token_type = self.KEYWORDS.get(self.buffer, TokenType.IDENTIFIER)
                self.add_token(token_type, self.buffer)
            elif self.state == State.B:
                 self.add_token(TokenType.INTEGER_CONST, self.buffer)
            elif self.state == State.D:
                 self.add_token(TokenType.FLOAT_CONST, self.buffer)
            # Обработка других состояний, если необходимо
            self.buffer = ""

        # Убедимся, что последний токен - EOF, если он еще не был добавлен
        if not self.tokens or self.tokens[-1].token_type != TokenType.EOF:
             self.add_token(TokenType.EOF)
             
        return self.tokens

```

**Логика работы `Lexer`:**

1.  **Инициализация**:
    *   `state` устанавливается в `State.S` (начальное).
    *   `buffer` очищается. Он используется для накопления символов многосимвольных лексем (идентификаторы, числа).
    *   `tokens` — пустой список для хранения результатов.
    *   `line`, `position`, `start_position` инициализируются для отслеживания местоположения в коде.
    *   `OPERATORS` и `KEYWORDS` — словари для быстрого определения типа лексемы по её строковому представлению.

2.  **Метод `analyze(input_string)`**:
    *   К входной строке `input_string` добавляется специальный символ конца файла `\0`. Это упрощает обработку конца ввода.
    *   Лексер проходит по строке символ за символом.
    *   **Работа конечного автомата**: В зависимости от текущего состояния (`self.state`) и текущего символа (`c`), лексер выполняет действия и переходит в новое состояние.
        *   **Состояние `S` (Начальное)**:
            *   Если `c` — буква: переход в `State.A` (идентификатор/ключевое слово). Символ добавляется в `buffer`.
            *   Если `c` — цифра: переход в `State.B` (целое число). Символ добавляется в `buffer`.
            *   Если `c` — оператор (из `self.OPERATORS`): создается токен оператора, лексер остается в `State.S`.
            *   Если `c` — пробел или табуляция: игнорируется, лексер остается в `State.S`.
            *   Если `c` — перевод строки (`\n`): номер строки `self.line` увеличивается, `self.position` сбрасывается, лексер остается в `State.S`.
            *   Если `c` — `\0` (конец файла): добавляется токен `EOF`, анализ завершается.
            *   Иначе: переход в `State.Z` (ошибка).
        *   **Состояние `A` (Идентификатор/Ключевое слово)**:
            *   Если `c` — буква или цифра: символ добавляется в `buffer`, лексер остается в `State.A`.
            *   Иначе (символ не является частью идентификатора):
                *   Содержимое `buffer` проверяется: является ли оно ключевым словом (по словарю `KEYWORDS`) или идентификатором.
                *   Создается соответствующий токен.
                *   `buffer` очищается.
                *   Лексер возвращается в `State.S`.
                *   **Важно**: текущий символ `c` не теряется, а обрабатывается заново уже в состоянии `S` (за счет `i -= 1` перед `continue` в старой реализации, или просто `continue` и обработка `c` на следующей итерации основного цикла `while i < len(input_string):` если `i` не декрементировать). В представленной выше реализации `i -= 1` используется для повторной обработки.
        *   **Состояние `B` (Целая часть числа)**:
            *   Если `c` — цифра: символ добавляется в `buffer`.
            *   Если `c` — точка (`.`): символ добавляется в `buffer`, переход в `State.C` (ожидание дробной части).
            *   Иначе: создается токен `INTEGER_CONST` из `buffer`, `buffer` очищается, переход в `State.S`, текущий символ `c` обрабатывается заново.
        *   **Состояние `C` (После точки)**:
            *   Если `c` — цифра: символ добавляется в `buffer`, переход в `State.D`.
            *   Иначе: ошибка (после точки должна идти цифра), переход в `State.Z`.
        *   **Состояние `D` (Дробная часть числа)**:
            *   Если `c` — цифра: символ добавляется в `buffer`.
            *   Иначе: создается токен `FLOAT_CONST` из `buffer`, `buffer` очищается, переход в `State.S`, текущий символ `c` обрабатывается заново.
        *   **Состояние `Z` (Ошибка)**: Генерируется исключение `RuntimeError` с информацией об ошибке.

3.  **Завершение анализа**:
    *   После прохода по всем символам, если в `buffer` остались накопленные символы (например, если файл закончился сразу после идентификатора), они обрабатываются для создания последнего токена.
    *   В список токенов обязательно добавляется `EOF`, если он не был добавлен ранее.
    *   Метод возвращает список `self.tokens`.

## Пример процесса токенизации

Рассмотрим строку кода NutScript: `int count = 10;`

1.  **`i`**: Лексер в состоянии `S`. Встречает `i`.
    *   `i` - буква. Переход в `State.A`. `buffer = "i"`.
2.  **`n`**: Лексер в состоянии `A`. Встречает `n`.
    *   `n` - буква. `buffer = "in"`.
3.  **`t`**: Лексер в состоянии `A`. Встречает `t`.
    *   `t` - буква. `buffer = "int"`.
4.  **` ` (пробел)**: Лексер в состоянии `A`. Встречает пробел.
    *   Пробел не буква и не цифра. Завершаем идентификатор/ключевое слово.
    *   `buffer` ("int") есть в `KEYWORDS`. Тип `TokenType.INT`.
    *   Добавляем `Token(TokenType.INT, "int", ...)`
    *   `buffer = ""`. Переход в `State.S`. Символ " " обрабатывается заново.
    *   Лексер в состоянии `S`. Встречает ` `. Игнорируем. Остаемся в `S`.
5.  **`c`**: Лексер в состоянии `S`. Встречает `c`.
    *   `c` - буква. Переход в `State.A`. `buffer = "c"`.
6.  ... (аналогично для `o`, `u`, `n`, `t`) ... `buffer = "count"`.
7.  **` ` (пробел)**: Лексер в состоянии `A`. Встречает пробел.
    *   Завершаем идентификатор. `buffer` ("count") нет в `KEYWORDS`. Тип `TokenType.IDENTIFIER`.
    *   Добавляем `Token(TokenType.IDENTIFIER, "count", ...)`
    *   `buffer = ""`. Переход в `State.S`. Символ " " обрабатывается заново (игнорируется).
8.  **`=`**: Лексер в состоянии `S`. Встречает `=`.
    *   `=` есть в `OPERATORS`. Тип `TokenType.ASSIGN`.
    *   Добавляем `Token(TokenType.ASSIGN, "=", ...)`
    *   Остаемся в `S`.
9.  **` ` (пробел)**: Лексер в состоянии `S`. Игнорируем.
10. **`1`**: Лексер в состоянии `S`. Встречает `1`.
    *   `1` - цифра. Переход в `State.B`. `buffer = "1"`.
11. **`0`**: Лексер в состоянии `B`. Встречает `0`.
    *   `0` - цифра. `buffer = "10"`.
12. **`;`**: Лексер в состоянии `B`. Встречает `;`.
    *   `;` не цифра и не точка. Завершаем число.
    *   Добавляем `Token(TokenType.INTEGER_CONST, "10", ...)`
    *   `buffer = ""`. Переход в `State.S`. Символ `;` обрабатывается заново.
    *   Лексер в состоянии `S`. Встречает `;`.
    *   `;` есть в `OPERATORS`. Тип `TokenType.SEMICOLON`.
    *   Добавляем `Token(TokenType.SEMICOLON, ";", ...)`
    *   Остаемся в `S`.
13. **`\0` (EOF)**: Лексер в состоянии `S`. Встречает конец файла.
    *   Добавляем `Token(TokenType.EOF, "", ...)`
    *   Анализ завершен.

**Итоговый список токенов (упрощенно):**
`INT("int")`, `IDENTIFIER("count")`, `ASSIGN("=")`, `INTEGER_CONST("10")`, `SEMICOLON(";")`, `EOF`

## Тестирование лексического анализатора

Для проверки корректности работы лексического анализатора используется скрипт `test_lexer.py`. Он принимает в качестве аргумента командной строки имя файла с исходным кодом NutScript, выполняет лексический анализ и выводит последовательность распознанных токенов с указанием их типа, значения (если есть), номера строки и позиции.

**Пример запуска:**
```bash
python test_lexer.py test1.kb
```

Это позволяет наглядно увидеть, как лексер обрабатывает различные конструкции языка и соответствует ли его поведение документации.

## Заключение

Лексический анализатор NutScript построен на основе конечного автомата, строго следующего спецификации языка. Он эффективно преобразует исходный код в поток лексем, подготавливая его для следующих этапов компиляции, таких как синтаксический и семантический анализ. Корректная работа лексического анализатора является критически важной для всего процесса компиляции.
